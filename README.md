---
title: AskMyDocs RAG Chatbot
emoji: ðŸ¤–ðŸ“„
colorFrom: indigo
colorTo: blue
sdk: streamlit
sdk_version: "1.38.0"
app_file: app.py
pinned: false
license: mit
---

# RAG Pipeline with FAISS + Streamlit (Local LLM via Ollama)

A **Retrieval-Augmented Generation (RAG)** system using FAISS for vector storage, Streamlit for a ChatGPT-style interface, and a local LLM via Ollama (Mistral) for private, on-device inference.

---

## Tech Stack
- **LLM (generation):** Mistral via Ollama  
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (HuggingFace)  
- **Vector DB:** FAISS (persisted locally)  
- **Frameworks:** LangChain (community + HuggingFace + Ollama integrations)  
- **UI:** Streamlit (chat interface + sidebar uploads)  
- **Loaders:** PyPDFLoader (PDF), TextLoader (TXT)  
- **Memory:** Session-level chat history (UI only, not passed to LLM for context)  

---

## Features
- Upload PDFs/TXTs â†’ automatic embeddings â†’ store in FAISS  
- Chat with your documents using a ChatGPT-style interface  
- Incremental knowledge growth; new uploads are added without overwriting  
- Local inference ensures privacy and no cloud dependency  
- Session-level chat history for UI continuity (each query is independent to the LLM)  

---

## Folder Structure

rag_project/
â”œâ”€ data/ # Uploaded PDFs/TXTs
â”œâ”€ index/ # FAISS vector DB (persistent)
â”œâ”€ ingest.py # Ingestion pipeline (embed + store docs)
â”œâ”€ query.py # Retrieval + generation
â”œâ”€ app.py # Streamlit chat app
â”œâ”€ requirements.txt # Dependencies
â””â”€ Procfile # Deployment configuration


---

## Getting Started

### Setup
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Install Ollama
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
ollama pull mistral     # or: ollama pull gemma:2b
```

### Run the App
```bash
streamlit run app.py
```

## Usage

1. Open the Streamlit app URL (typically `http://localhost:8501`)
2. Use "Manage Documents" in the sidebar to upload PDFs/TXTs
3. Ask questions in the chat; answers are grounded in uploaded documents
4. Knowledge base grows incrementally as new documents are added

## Implementation Details

- **Ingestion:** PyPDFLoader & TextLoader â†’ HuggingFace embeddings â†’ FAISS
- **Retrieval + Generation:** FAISS retriever (top-3 chunks) â†’ LangChain prompt â†’ Ollama Mistral LLM
- **UI Memory:** Streamlit session state maintains chat history for display only

## Achievements

- âœ… Fully functional RAG pipeline with FAISS
- âœ… ChatGPT-style UI for Q&A over documents
- âœ… Incremental knowledge base growth (no reindexing required)
- âœ… Fully local inference (privacy-friendly, no cloud dependency)
- âœ… Support for both PDF and TXT documents
